# standard imports
import numpy as np
import random
from os.path import exists

# RL imports
import gymnasium as gym
import minigrid
from minigrid.wrappers import *
from collections import namedtuple, deque

# torch imports
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# tensorboard imports
from torch.utils.tensorboard import SummaryWriter

# import functions
from utils import *
from train import *

### TESTING HYPERPARAMETERS
# Evaluation hyperparameter
evalEpisodes = 1000  # Number of episodes to be used for evaluation


# Set the random seed for reproducible experiments
random.seed(5)


max_steps = env.max_steps


def evaluate_model():
    # Evaluation loop
    print("Starting evaluation...")
    # Set the path to the saved model
    model_path = "models/trained_model.pth"

    # policy_net = DQN(inputSize, numActions, hiddenLayerSize)
    # update policy_net with model
    policy_net.load_state_dict(torch.load(model_path))

    # Make the environment
    env = create_minigrid_environment()

    # use wrapper to only extract observation
    env = ImgObsWrapper(env)

    # Initialize counters for evaluation metrics
    finishCounter = 0.0
    totalSteps = 0.0
    totalReward = 0.0

    # Run the evaluation loop
    for e in range(evalEpisodes):
        # Initialize the environment and state
        currentObs, _ = env.reset()
        currentState = preprocess(currentObs)

        # the main RL loop
        for i in range(0, max_steps):
            # Select and perform an action
            action = select_action(currentState)
            a = action.item()

            # Take action 'a', receive reward 'reward', and observe the next state 'obs'
            # 'done' indicates if the termination state was reached
            obs, reward, done, truncated, info = env.step(a)

            if done or truncated:
                # Observe the new state
                nextState = None
            else:
                nextState = preprocess(obs)

            if done or truncated:
                totalReward += reward
                totalSteps += env.step_count
                if done:
                    print(
                        "Finished evaluation episode %d with reward %f, %d steps, reaching the goal"
                        % (e, reward, env.step_count)
                    )
                    finishCounter += 1
                if truncated:
                    print(
                        "Failed evaluation episode %d with reward %f, %d steps"
                        % (e, reward, env.step_count)
                    )
                break

            # Move to the next state
            currentState = nextState

    # Print a summary of the evaluation results
    print(
        "Completion rate %.2f with an average reward %0.4f and average steps %0.2f"
        % (
            finishCounter / evalEpisodes,
            totalReward / evalEpisodes,
            totalSteps / evalEpisodes,
        )
    )
    print("Finished evaluation!")


if __name__ == "__main__":
    evaluate_model()
